{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [str(t) for t in TextBlob(text).tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_key(text):\n",
    "    \"\"\"Convert text -> normalized index key.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "\n",
    "    text = text.replace('.', '')\n",
    "    text = re.sub('[,-]', ' ', text)\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.children = defaultdict(list)\n",
    "        self.final = set()\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"Index key.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __call__(self):\n",
    "        \"\"\"Accept fn.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __hash__(self):\n",
    "        \"\"\"Used for pairwise equality checks.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return type(self) == type(other) and hash(self) == hash(other)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        self.final.update(other.final)\n",
    "        return self\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return [c for c in self.children[make_key(token)] if c(token)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return sum([1 + len(n) for sibs in self.children.values() for n in sibs])\n",
    "        \n",
    "    def insert(self, children):\n",
    "\n",
    "        head = children[0]\n",
    "\n",
    "        key = make_key(str(head))\n",
    "\n",
    "        merged = False\n",
    "        for other in self.children[key]:\n",
    "            if head == other:\n",
    "                head = other + head\n",
    "                merged = True\n",
    "                break\n",
    "                \n",
    "        print(head, merged)\n",
    "\n",
    "        if not merged:\n",
    "            self.children[key].append(head)\n",
    "            \n",
    "        if len(children) > 1:\n",
    "            head.insert(children[1:])\n",
    "            \n",
    "    def query(self, tokens):\n",
    "        \n",
    "        matches = self[tokens[0]]\n",
    "        \n",
    "        if len(tokens) == 1:\n",
    "            for match in matches:\n",
    "                yield from match.final\n",
    "                \n",
    "        elif len(tokens) > 1:\n",
    "            for match in matches:\n",
    "                yield from match.query(tokens[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RootNode(TrieNode):\n",
    "    \n",
    "    def insert(self, id, children):\n",
    "        children[-1].final.add(id)\n",
    "        super().insert(children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    " class Token(TrieNode):\n",
    "    \n",
    "    def __init__(self, token, ignore_case=True, scrub_re='\\.'):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.ignore_case = ignore_case\n",
    "        self.scrub_re = scrub_re\n",
    "        \n",
    "        self.token = token\n",
    "        self.token_clean = self._clean(token)\n",
    "        \n",
    "    def _clean(self, token):\n",
    "        \n",
    "        if self.ignore_case:\n",
    "            token = token.lower()\n",
    "            \n",
    "        if self.scrub_re:\n",
    "            token = re.sub(self.scrub_re, '', token)\n",
    "            \n",
    "        return token\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.token\n",
    "    \n",
    "    def __call__(self, input_token):\n",
    "        return self._clean(input_token) == self.token_clean\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.token_clean, self.ignore_case, self.scrub_re))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '%s<%s>' % (self.__class__.__name__, self.token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = RootNode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los False\n",
      "Angeles False\n",
      "CA False\n",
      "Los True\n",
      "Angeles True\n",
      "California False\n"
     ]
    }
   ],
   "source": [
    "idx.insert(1, [Token('Los'), Token('Angeles'), Token('CA', ignore_case=False)])\n",
    "idx.insert(1, [Token('Los'), Token('Angeles'), Token('California')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(idx.query(tokenize('Los Angeles CA')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(idx.query(tokenize('Los Angeles California')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(idx.query(tokenize('Los Angeles ca')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
