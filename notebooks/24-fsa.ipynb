{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from collections import defaultdict, Counter\n",
    "from textblob import TextBlob\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "from litecoder.db import City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.style.use('seaborn-muted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [str(t) for t in TextBlob(text).tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyify(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-z0-9]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "    \n",
    "    def __init__(self, token, ignore_case=True, scrub_re='\\.'):\n",
    "        \n",
    "        self.ignore_case = ignore_case\n",
    "        self.scrub_re = scrub_re\n",
    "        \n",
    "        self.token = token\n",
    "        self.token_clean = self._clean(token)\n",
    "        \n",
    "    def _clean(self, token):\n",
    "        \n",
    "        if self.ignore_case:\n",
    "            token = token.lower()\n",
    "            \n",
    "        if self.scrub_re:\n",
    "            token = re.sub(self.scrub_re, '', token)\n",
    "            \n",
    "        return token\n",
    "    \n",
    "    def __call__(self, input_token):\n",
    "        return self._clean(input_token) == self.token_clean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '%s<%s>' % (self.__class__.__name__, self.token_clean)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        # TODO: Class identifier?\n",
    "        return hash((self.token_clean, self.ignore_case, self.scrub_re))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return hash(self) == hash(other)\n",
    "    \n",
    "    def label(self):\n",
    "        return '<%s>' % self.token_clean\n",
    "    \n",
    "    def key(self):\n",
    "        return keyify(self.token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoFSA(nx.MultiDiGraph):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._next_id = 0\n",
    "        \n",
    "    def add_node(self, node, **kwargs):\n",
    "        defaults = dict(final=set())\n",
    "        super().add_node(node, **{**defaults, **kwargs})\n",
    "        \n",
    "    def add_edge(self, u, v, entity=None, **kwargs):\n",
    "        \"\"\"Ensure edges have non-empty entity sets.\n",
    "        \"\"\"\n",
    "        defaults = dict(accept_fn=None, entities=set([entity]), label=None)\n",
    "        kwargs = {**defaults, **kwargs}\n",
    "        \n",
    "        if not len(kwargs['entities']) > 0:\n",
    "            raise RuntimeError('All edges must have a non-empty entity set.')\n",
    "        \n",
    "        super().add_edge(u, v, **{**defaults, **kwargs})\n",
    "        \n",
    "    def next_node(self):\n",
    "        \"\"\"Get next integer node id, counting up.\n",
    "        \"\"\"\n",
    "        node = self._next_id\n",
    "        \n",
    "        self.add_node(node)\n",
    "        self._next_id += 1\n",
    "        \n",
    "        return node\n",
    "        \n",
    "    def add_token(self, accept_fn, entity, parent=None, optional=False):\n",
    "        \"\"\"Register a token transition.\n",
    "        \"\"\"\n",
    "        s1 = parent if parent else self.next_node()\n",
    "        s2 = self.next_node()\n",
    "        \n",
    "        self.add_edge(\n",
    "            s1, s2, entity,\n",
    "            accept_fn=accept_fn,\n",
    "            label=accept_fn.label(),\n",
    "        )\n",
    "        \n",
    "        last_node = s2\n",
    "        \n",
    "        # Add skip links if optional.\n",
    "        if optional:\n",
    "            s3 = self.next_node()\n",
    "            self.add_edge(s2, s3, entity, label='ε')\n",
    "            self.add_edge(s1, s3, entity, label='ε')\n",
    "            last_node = s3\n",
    "        \n",
    "        return last_node\n",
    "    \n",
    "    def set_final(self, state, entity):\n",
    "        self.node[state]['final'].add(entity)\n",
    "\n",
    "    def start_nodes(self):\n",
    "        return [n for n in self.nodes() if self.in_degree(n) == 0]\n",
    "    \n",
    "    def inner_nodes(self):\n",
    "        return [n for n in self.nodes() if self.out_degree(n) > 0]\n",
    "        \n",
    "    def end_nodes(self):\n",
    "        return [n for n in self.nodes() if self.out_degree(n) == 0]\n",
    "    \n",
    "    def _merge_nodes(self, u, v):\n",
    "        \"\"\"Merge two leaf nodes.\n",
    "        \"\"\"\n",
    "        # Add v finals -> u finals.\n",
    "        self.node[u]['final'].update(g.node[v]['final'])\n",
    "        \n",
    "        # Redirect in edges.\n",
    "        for s, _, data in g.in_edges(v, data=True):\n",
    "            g.add_edge(s, u, **data)\n",
    "            \n",
    "        # Redirect out edges.\n",
    "        for _, t, data in g.out_edges(v, data=True):\n",
    "            g.add_edge(u, t, **data)\n",
    "            \n",
    "        self.remove_node(v)\n",
    "        \n",
    "    def _merge_edges(self, u, v, k1, k2):\n",
    "        \"\"\"Merge two edges between a pair of nodes.\n",
    "        \"\"\"\n",
    "        # Add k2 entities -> k1 entities.\n",
    "        self[u][v][k1]['entities'].update(self[u][v][k2]['entities'])\n",
    "        \n",
    "        self.remove_edge(u, v, k2)\n",
    "        \n",
    "    def _end_node_merge_key(self, node):\n",
    "        \"\"\"Build merge key for end node.\n",
    "        \"\"\"\n",
    "        return frozenset([\n",
    "            data.get('accept_fn')\n",
    "            for _, _, data in self.in_edges(node, data=True)\n",
    "        ])\n",
    "    \n",
    "    def _inner_node_merge_key(self, node):\n",
    "        \"\"\"Build merge key for inner node.\n",
    "        \"\"\"\n",
    "        out_edges = frozenset([\n",
    "            data.get('accept_fn')\n",
    "            for _, _, data in self.out_edges(node, data=True)\n",
    "        ])\n",
    "        \n",
    "        descendants = frozenset(nx.descendants(self, node))\n",
    "        \n",
    "        return (out_edges, descendants)\n",
    "    \n",
    "    def reduce_end_nodes(self):\n",
    "        \"\"\"Reduce all redundant end nodes.\n",
    "        \"\"\"\n",
    "        seen = {}\n",
    "        for v in self.end_nodes():\n",
    "            \n",
    "            key = self._end_node_merge_key(v)\n",
    "            u = seen.get(key)\n",
    "            \n",
    "            if u:\n",
    "                self._merge_nodes(u, v)  \n",
    "            else:\n",
    "                seen[key] = v\n",
    "                \n",
    "    def _reduce_inner_nodes_iter(self):\n",
    "        \"\"\"Perform one iteration of inner node reduction.\n",
    "        \"\"\"\n",
    "        seen = {}\n",
    "        for v in self.inner_nodes():\n",
    "            \n",
    "            key = self._inner_node_merge_key(v)\n",
    "            u = seen.get(key)\n",
    "            \n",
    "            if u:\n",
    "                self._merge_nodes(u, v)\n",
    "            else:\n",
    "                seen[key] = v\n",
    "                    \n",
    "    def reduce_inner_nodes(self):\n",
    "        \"\"\"Reduce inner nodes until no more merges are possible.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            nc1 = len(self.nodes)\n",
    "            self._reduce_inner_nodes_iter()\n",
    "            nc2 = len(self.nodes)\n",
    "            if nc2 == nc1:\n",
    "                break\n",
    "                \n",
    "    def _reduce_node_out_edges(self, node):\n",
    "        \"\"\"Reduce out edges from node.\n",
    "        \"\"\"\n",
    "        out_edges = list(self.out_edges(node, data=True, keys=True))\n",
    "        \n",
    "        seen = {}\n",
    "        for s, t, k2, data in out_edges:\n",
    "            \n",
    "            key = (t, data['accept_fn'])\n",
    "            k1 = seen.get(key)\n",
    "            \n",
    "            if k1 is not None:\n",
    "                self._merge_edges(s, t, k1, k2)\n",
    "\n",
    "            else:\n",
    "                seen[key] = k2\n",
    "                \n",
    "    def reduce_out_edges(self):\n",
    "        \"\"\"Reduce all out edges.\n",
    "        \"\"\"\n",
    "        for node in self.nodes():\n",
    "            self._reduce_node_out_edges(node)\n",
    "            \n",
    "    def start_index_kv_iter(self):\n",
    "        \"\"\"Generate key -> start node pairs.\n",
    "        \"\"\"\n",
    "        for node in self.start_nodes():\n",
    "            for _, _, data in self.out_edges(node, data=True):\n",
    "                if data['accept_fn']:\n",
    "                    yield data['accept_fn'].key(), node\n",
    "                    \n",
    "    def start_index(self):\n",
    "        \"\"\"Map key -> start nodes.\n",
    "        \"\"\"\n",
    "        idx = defaultdict(list)\n",
    "        for k, n in self.start_index_kv_iter():\n",
    "            idx[k].append(n)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(g):\n",
    "    dot = nx.drawing.nx_pydot.to_pydot(g)\n",
    "    dot.set_rankdir('LR')\n",
    "    display(Image(dot.create_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.27s/it]\u001b[A\n",
      "378it [00:01, 275.98it/s]\u001b[A\n",
      "745it [00:01, 506.88it/s]\u001b[A\n",
      "1072it [00:01, 654.34it/s]\u001b[A\n",
      "1356it [00:01, 780.20it/s]\u001b[A\n",
      "1724it [00:01, 937.85it/s]\u001b[A\n",
      "2082it [00:01, 1073.95it/s]\u001b[A\n",
      "2452it [00:02, 1202.69it/s]\u001b[A\n",
      "2807it [00:02, 1312.28it/s]\u001b[A\n",
      "3134it [00:02, 1335.30it/s]\u001b[A\n",
      "3502it [00:02, 1431.02it/s]\u001b[A\n",
      "3871it [00:02, 1519.55it/s]\u001b[A\n",
      "4240it [00:02, 1601.45it/s]\u001b[A\n",
      "4600it [00:02, 1674.07it/s]\u001b[A\n",
      "4956it [00:02, 1740.34it/s]\u001b[A\n",
      "5304it [00:03, 1714.78it/s]\u001b[A\n",
      "5667it [00:03, 1774.78it/s]\u001b[A\n",
      "6035it [00:03, 1832.47it/s]\u001b[A\n",
      "6380it [00:03, 1880.12it/s]\u001b[A\n",
      "6733it [00:03, 1927.29it/s]\u001b[A\n",
      "7072it [00:03, 1967.17it/s]\u001b[A\n",
      "7454it [00:03, 2017.26it/s]\u001b[A\n",
      "7828it [00:03, 2062.59it/s]\u001b[A\n",
      "8186it [00:04, 2010.53it/s]\u001b[A\n",
      "8483it [00:04, 2033.49it/s]\u001b[A\n",
      "8834it [00:04, 2067.99it/s]\u001b[A\n",
      "9187it [00:04, 2101.28it/s]\u001b[A\n",
      "9536it [00:04, 2132.26it/s]\u001b[A\n",
      "9880it [00:04, 2160.66it/s]\u001b[A\n",
      "10231it [00:04, 2189.25it/s]\u001b[A\n",
      "10575it [00:04, 2215.66it/s]\u001b[A\n",
      "10933it [00:04, 2243.71it/s]\u001b[A\n",
      "11296it [00:04, 2271.42it/s]\u001b[A\n",
      "11647it [00:05, 2293.92it/s]\u001b[A\n",
      "11994it [00:05, 2221.70it/s]\u001b[A\n",
      "12302it [00:05, 2237.40it/s]\u001b[A\n",
      "12668it [00:05, 2262.67it/s]\u001b[A\n",
      "13039it [00:05, 2287.97it/s]\u001b[A\n",
      "13386it [00:05, 2308.35it/s]\u001b[A\n",
      "13741it [00:05, 2329.25it/s]\u001b[A\n",
      "14085it [00:05, 2347.74it/s]\u001b[A\n",
      "14446it [00:06, 2368.40it/s]\u001b[A\n",
      "14813it [00:06, 2389.42it/s]\u001b[A\n",
      "15178it [00:06, 2409.31it/s]\u001b[A\n",
      "15535it [00:06, 2426.36it/s]\u001b[A\n",
      "15889it [00:06, 2439.52it/s]\u001b[A\n",
      "16233it [00:06, 2452.48it/s]\u001b[A\n",
      "16873it [00:07, 2375.92it/s]\n",
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/dclure/Projects/litecoder/env/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/local/bin/../Cellar/python/3.6.5/bin/../Frameworks/Python.framework/Versions/3.6/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "20000it [00:08, 2473.21it/s]\n"
     ]
    }
   ],
   "source": [
    "g = GeoFSA()\n",
    "\n",
    "for city in tqdm(City.query.filter(City.country_iso=='US').limit(20000)):\n",
    "    \n",
    "    entity = (City.__tablename__, city.wof_id)\n",
    "    \n",
    "    name_tokens = tokenize(city.name)\n",
    "    state_tokens = tokenize(city.name_a1)\n",
    "    \n",
    "    # City name\n",
    "    parent = None\n",
    "    for token in name_tokens:\n",
    "        parent = g.add_token(Token(token), entity, parent)\n",
    "        \n",
    "    # Optional comma\n",
    "    comma = g.add_token(Token(','), entity, parent, optional=True)\n",
    "    \n",
    "    # State name\n",
    "    parent = comma\n",
    "    for token in state_tokens:\n",
    "        parent = g.add_token(Token(token), entity, parent)\n",
    "\n",
    "    g.set_final(parent, entity)\n",
    "        \n",
    "    # Or, state abbr\n",
    "    leaf = g.add_token(Token(city.us_state_abbr), entity, comma)\n",
    "    g.set_final(leaf, entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.reduce_end_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.reduce_inner_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.reduce_out_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matcher:\n",
    "    \n",
    "    def __init__(self, fsa):\n",
    "        self.fsa = fsa\n",
    "        self._start_index = fsa.start_index()\n",
    "        self._states = set()\n",
    "        \n",
    "    def _get_next_states(self, start_state, token, visited=None):\n",
    "        \n",
    "        if not visited:\n",
    "            visited = set()\n",
    "            \n",
    "        visited.add(start_state)\n",
    "        \n",
    "        next_states = set()\n",
    "        for _, state, data in self.fsa.out_edges(start_state, data=True):\n",
    "            \n",
    "            accept_fn = data['accept_fn']\n",
    "            \n",
    "            # If non-empty transition, evaluate input.\n",
    "            if accept_fn:\n",
    "                if accept_fn(token):\n",
    "                    next_states.add(state)\n",
    "                \n",
    "            # Recurisvely resolve epsilons.\n",
    "            elif state not in visited:\n",
    "                next_states.update(self._get_next_states(state, token, visited))\n",
    "                \n",
    "        return next_states\n",
    "    \n",
    "    def __call__(self, token):\n",
    "\n",
    "        if not self._states:\n",
    "            self._states.update(self._start_index[keyify(token)])\n",
    "        \n",
    "        next_states = set()\n",
    "        for state in self._states:\n",
    "            next_states.update(self._get_next_states(state, token))\n",
    "            \n",
    "        self._states = next_states\n",
    "\n",
    "        print(self._states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Matcher(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{116992, 127488, 8454, 85766, 62986, 41998, 41486, 40466, 127763, 128278, 45592, 103704, 108315, 26142, 45599, 65058, 802, 61731, 123178, 19756, 23085, 121904, 79922, 54072, 125240, 66106, 25659, 37948, 27709, 77115, 25154, 103746, 78406, 126023, 55366, 66886, 88650, 79180, 90189, 89420, 128845, 48208, 64081, 15698, 42067, 91219, 128600, 67416, 121948, 112734, 126303, 111712, 39779, 25188, 125285, 124772, 51559, 43624, 67176, 69738, 55144, 85865, 623, 53359, 49007, 54899, 110197, 47477, 12152, 86137, 52089, 50299, 81786, 25213, 47998, 86143, 88441, 83585, 128897, 79745, 39556, 80517, 49798, 118661, 128390, 57736, 37770, 109195, 85643, 49546, 87694, 101772, 72591, 47762, 50835, 128662, 42648, 44696, 107672, 50075, 125084, 108187, 54943, 48290, 40866, 50594, 51106, 110501, 125348, 102056, 125858, 126889, 125355, 99755, 69547, 82606, 431, 47792, 110510, 89778, 50608, 76208, 100022, 74375, 80313, 48826, 53435, 54459, 111038, 74945, 66242, 43457, 75714, 3524, 1990, 122306, 124867, 29898, 115660, 101583, 47828, 128469, 73177, 8154, 11995, 60890, 126425, 4062, 78558, 45280, 51425, 91109, 122342, 51943, 90856, 76009, 25576, 85480, 61676, 50155, 128751, 45043, 125430, 27126, 3833, 80124, 107773, 25087}\n",
      "CPU times: user 3.09 ms, sys: 125 µs, total: 3.21 ms\n",
      "Wall time: 3.17 ms\n"
     ]
    }
   ],
   "source": [
    "%time m('new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{512, 17408, 111749, 120326, 100999, 30474, 116875, 29586, 6932, 2455, 91545, 66588, 81055, 10656, 32, 21028, 26791, 27180, 29614, 80175, 107824, 79153, 12850, 81597, 101438, 33474, 100552, 114252, 25037, 12238, 103761, 98644, 103894, 15577, 29913, 108252, 49633, 31842, 29412, 79460, 32871, 12647, 65002, 13045, 26486, 5879, 505, 79103}\n",
      "{2, 95164, 26461, 103762}\n",
      "{3, 95165, 26462}\n",
      "{6}\n"
     ]
    }
   ],
   "source": [
    "m('pine')\n",
    "m('RIDGE')\n",
    "m(',')\n",
    "m('AL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
